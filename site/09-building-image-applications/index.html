<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Building Image Generation Applications - genaibeginner</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Building Image Generation Applications";
        var mkdocs_page_input_path = "09-building-image-applications\\README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> genaibeginner
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../00-course-setup/translations/cn/">课程介绍和学习环境设置</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../01-introduction-to-genai/translations/cn/">第一章：生成式人工智能和 LLMs 介绍</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../02-exploring-and-comparing-different-llms/translations/cn/">第二章：探索和比较不同的 LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../03-using-generative-ai-responsibly/translations/cn/">第三章：负责任地使用生成式人工智能</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../04-prompt-engineering-fundamentals/translations/cn/">第四章：提示工程基础</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../05-advanced-prompts/translations/cn/">第五章：创建高级的提示工程技巧</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../06-text-generation-apps/translations/cn/">第六章：创建文本生成应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../07-building-chat-applications/translations/cn/">第七章：创建聊天应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../08-building-search-applications/translations/cn/">第八章：创建搜索应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="translations/cn/">第九章：创建图像应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../10-building-low-code-ai-applications/translations/cn/">第十章：创建低代码AI应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../11-integrating-with-function-calling/translations/cn/">第十一章：集成函数调用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../12-designing-ux-for-ai-applications/translations/cn/">第十二章：为人工智能应用程序设计用户体验</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13-securing-ai-applications/translations/cn/">第十三章：保护AI应用程序</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../14-the-generative-ai-application-lifecycle/translations/cn/">第十四章：生成式AI应用生命周期</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../15-rag-and-vector-databases/translations/cn/">第十五章：检索增强生成和向量数据库</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../16-open-source-models/translations/tw/">第十六章：开源模型</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../17-ai-agents/translations/tw/">第十七章：AI Agent</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../18-fine-tuning/translations/tw/">第十八章：微调</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../19-slm/">第十九章：SLM模型</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../20-mistral/">第二十章：Mistral的模型</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../21-meta/">第二十一章：Meta的模型</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">genaibeginner</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Building Image Generation Applications</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="building-image-generation-applications">Building Image Generation Applications</h1>
<p><a href="https://aka.ms/gen-ai-lesson9-gh?WT.mc_id=academic-105485-koreyst"><img alt="Building Image Generation Applications" src="images/09-lesson-banner.png?WT.mc_id=academic-105485-koreyst" /></a></p>
<p>There's more to LLMs than text generation. It's also possible to generate images from text descriptions. Having images as a modality can be highly useful in a number of areas from MedTech, architecture, tourism, game development and more. In this chapter, we will look into the two most popular image generation models, DALL-E and Midjourney.</p>
<h2 id="introduction">Introduction</h2>
<p>In this lesson, we will cover:</p>
<ul>
<li>Image generation and why it's useful.</li>
<li>DALL-E and Midjourney, what they are, and how they work.</li>
<li>How you would build an image generation app.</li>
</ul>
<h2 id="learning-goals">Learning Goals</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
<li>Build an image generation application.</li>
<li>Define boundaries for your application with meta prompts.</li>
<li>Work with DALL-E and Midjourney.</li>
</ul>
<h2 id="why-build-an-image-generation-application">Why build an image generation application?</h2>
<p>Image generation applications are a great way to explore the capabilities of Generative AI. They can be used for, for example:</p>
<ul>
<li>
<p><strong>Image editing and synthesis</strong>. You can generate images for a variety of use cases, such as image editing and image synthesis.</p>
</li>
<li>
<p><strong>Applied to a variety of industries</strong>. They can also be used to generate images for a variety of industries like Medtech, Tourism, Game development and more.</p>
</li>
</ul>
<h2 id="scenario-edu4all">Scenario: Edu4All</h2>
<p>As part of this lesson, we will continue to work with our startup, Edu4All, in this lesson. The students will create images for their assessments, exactly what images is up to the students, but they could be illustrations for their own fairytale or create a new character for their story or help them visualize their ideas and concepts.</p>
<p>Here's what Edu4All's students could generate for example if they're working in class on monuments:</p>
<p><img alt="Edu4All startup, class on monuments, Eiffel Tower" src="images/startup.png?WT.mc_id=academic-105485-koreyst" /></p>
<p>using a prompt like</p>
<blockquote>
<p>"Dog next to Eiffel Tower in early morning sunlight"</p>
</blockquote>
<h2 id="what-is-dall-e-and-midjourney">What is DALL-E and Midjourney?</h2>
<p><a href="https://openai.com/dall-e-2?WT.mc_id=academic-105485-koreyst">DALL-E</a> and <a href="https://www.midjourney.com/?WT.mc_id=academic-105485-koreyst">Midjourney</a> are two of the most popular image generation models, they allow you to use prompts to generate images.</p>
<h3 id="dall-e">DALL-E</h3>
<p>Let's start with DALL-E, which is a Generative AI model that generates images from text descriptions.</p>
<blockquote>
<p><a href="https://towardsdatascience.com/openais-dall-e-and-clip-101-a-brief-introduction-3a4367280d4e?WT.mc_id=academic-105485-koreyst">DALL-E is a combination of two models, CLIP and diffused attention</a>.</p>
</blockquote>
<ul>
<li>
<p><strong>CLIP</strong>, is a model that generates embeddings, which are numerical representations of data, from images and text.</p>
</li>
<li>
<p><strong>Diffused attention</strong>, is a model that generates images from embeddings. DALL-E is trained on a dataset of images and text and can be used to generate images from text descriptions. For example, DALL-E can be used to generate images of a cat in a hat, or a dog with a mohawk.</p>
</li>
</ul>
<h3 id="midjourney">Midjourney</h3>
<p>Midjourney works in a similar way to DALL-E, it generates images from text prompts. Midjourney, can also be used to generate images using prompts like “a cat in a hat”, or a “dog with a mohawk”.</p>
<p><img alt="Image generated by Midjourney, mechanical pigeon" src="https://upload.wikimedia.org/wikipedia/commons/thumb/8/8c/Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png/440px-Rupert_Breheny_mechanical_dove_eca144e7-476d-4976-821d-a49c408e4f36.png?WT.mc_id=academic-105485-koreyst" />
<em>Image cred Wikipedia, image generated by Midjourney</em></p>
<h2 id="how-does-dall-e-and-midjourney-work">How does DALL-E and Midjourney Work</h2>
<p>First, <a href="https://arxiv.org/pdf/2102.12092.pdf?WT.mc_id=academic-105485-koreyst">DALL-E</a>. DALL-E is a Generative AI model based on the transformer architecture with an <em>autoregressive transformer</em>.</p>
<p>An <em>autoregressive transformer</em> defines how a model generates images from text descriptions, it generates one pixel at a time, and then uses the generated pixels to generate the next pixel. Passing through multiple layers in a neural network, until the image is complete.</p>
<p>With this process, DALL-E, controls attributes, objects, characteristics, and more in the image it generates. However, DALL-E 2 and 3 have more control over the generated image.</p>
<h2 id="building-your-first-image-generation-application">Building your first image generation application</h2>
<p>So what does it take to build an image generation application? You need the following libraries:</p>
<ul>
<li><strong>python-dotenv</strong>, you're highly recommended to use this library to keep your secrets in a <em>.env</em> file away from the code.</li>
<li><strong>openai</strong>, this library is what you will use to interact with the OpenAI API.</li>
<li><strong>pillow</strong>, to work with images in Python.</li>
<li>
<p><strong>requests</strong>, to help you make HTTP requests.</p>
</li>
<li>
<p>Create a file <em>.env</em> with the following content:</p>
</li>
</ul>
<p><code>text
   AZURE_OPENAI_ENDPOINT=&lt;your endpoint&gt;
   AZURE_OPENAI_API_KEY=&lt;your key&gt;</code></p>
<p>Locate this information in Azure Portal for your resource in the "Keys and Endpoint" section.</p>
<ol>
<li>Collect the above libraries in a file called <em>requirements.txt</em> like so:</li>
</ol>
<p><code>text
   python-dotenv
   openai
   pillow
   requests</code></p>
<ol>
<li>Next, create virtual environment and install the libraries:</li>
</ol>
<p><code>bash
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt</code></p>
<p>For Windows, use the following commands to create and activate your virtual environment:</p>
<p><code>bash
   python3 -m venv venv
   venv\Scripts\activate.bat</code></p>
<ol>
<li>Add the following code in file called <em>app.py</em>:</li>
</ol>
<p>```python
   import openai
   import os
   import requests
   from PIL import Image
   import dotenv</p>
<p># import dotenv
   dotenv.load_dotenv()</p>
<p># Get endpoint and key from environment variables
   openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']
   openai.api_key = os.environ['AZURE_OPENAI_API_KEY']</p>
<p># Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)
   openai.api_version = '2023-06-01-preview'
   openai.api_type = 'azure'</p>
<p>try:
       # Create an image by using the image generation API
       generation_response = openai.Image.create(
           prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
           size='1024x1024',
           n=2,
           temperature=0,
       )
       # Set the directory for the stored image
       image_dir = os.path.join(os.curdir, 'images')</p>
<pre><code>   # If the directory doesn't exist, create it
   if not os.path.isdir(image_dir):
       os.mkdir(image_dir)

   # Initialize the image path (note the filetype should be png)
   image_path = os.path.join(image_dir, 'generated-image.png')

   # Retrieve the generated image
   image_url = generation_response["data"][0]["url"]  # extract image URL from response
   generated_image = requests.get(image_url).content  # download the image
   with open(image_path, "wb") as image_file:
       image_file.write(generated_image)

   # Display the image in the default image viewer
   image = Image.open(image_path)
   image.show()
</code></pre>
<p># catch exceptions
   except openai.InvalidRequestError as err:
       print(err)</p>
<p>```</p>
<p>Let's explain this code:</p>
<ul>
<li>First, we import the libraries we need, including the OpenAI library, the dotenv library, the requests library, and the Pillow library.</li>
</ul>
<p><code>python
  import openai
  import os
  import requests
  from PIL import Image
  import dotenv</code></p>
<ul>
<li>Next, we load the environment variables from the <em>.env</em> file.</li>
</ul>
<p><code>python
  # import dotenv
  dotenv.load_dotenv()</code></p>
<ul>
<li>After that, we set the endpoint, key for the OpenAI API, version and type.</li>
</ul>
<p>```python
  # Get endpoint and key from environment variables
  openai.api_base = os.environ['AZURE_OPENAI_ENDPOINT']
  openai.api_key = os.environ['AZURE_OPENAI_API_KEY']</p>
<p># add version and type, Azure specific
  openai.api_version = '2023-06-01-preview'
  openai.api_type = 'azure'
  ```</p>
<ul>
<li>Next, we generate the image:</li>
</ul>
<p><code>python
  # Create an image by using the image generation API
  generation_response = openai.Image.create(
      prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
      size='1024x1024',
      n=2,
      temperature=0,
  )</code></p>
<p>The above code responds with a JSON object that contains the URL of the generated image. We can use the URL to download the image and save it to a file.</p>
<ul>
<li>Lastly, we open the image and use the standard image viewer to display it:</li>
</ul>
<p><code>python
  image = Image.open(image_path)
  image.show()</code></p>
<h3 id="more-details-on-generating-the-image">More details on generating the image</h3>
<p>Let's look at the code that generates the image in more detail:</p>
<pre><code class="language-python">generation_response = openai.Image.create(
        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
        size='1024x1024',
        n=2,
        temperature=0,
    )
</code></pre>
<ul>
<li><strong>prompt</strong>, is the text prompt that is used to generate the image. In this case, we're using the prompt "Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils".</li>
<li><strong>size</strong>, is the size of the image that is generated. In this case, we're generating an image that is 1024x1024 pixels.</li>
<li><strong>n</strong>, is the number of images that are generated. In this case, we're generating two images.</li>
<li><strong>temperature</strong>, is a parameter that controls the randomness of the output of a Generative AI model. The temperature is a value between 0 and 1 where 0 means that the output is deterministic and 1 means that the output is random. The default value is 0.7.</li>
</ul>
<p>There are more things you can do with images that we will cover in the next section.</p>
<h2 id="additional-capabilities-of-image-generation">Additional capabilities of image generation</h2>
<p>You've seen so far how we were able to generate an image using a few lines in Python. However, there are more things you can do with images.</p>
<p>You can also do the following:</p>
<ul>
<li><strong>Perform edits</strong>. By providing an existing image a mask and a prompt, you can alter an image. For example, you can add something to a portion of an image. Imagine our bunny image, you can add a hat to the bunny. How you would do that is by providing the image, a mask (identifying the part of the area for the change) and a text prompt to say what should be done.</li>
</ul>
<p><code>python
  response = openai.Image.create_edit(
    image=open("base_image.png", "rb"),
    mask=open("mask.png", "rb"),
    prompt="An image of a rabbit with a hat on its head.",
    n=1,
    size="1024x1024"
  )
  image_url = response['data'][0]['url']</code></p>
<p>The base image would only contain the rabbit but the final image would have the hat on the rabbit.</p>
<ul>
<li><strong>Create variations</strong>. The idea is that you take an existing image and ask that variations are created. To create a variation, you provide an image and a text prompt and code like so:</li>
</ul>
<p><code>python
  response = openai.Image.create_variation(
    image=open("bunny-lollipop.png", "rb"),
    n=1,
    size="1024x1024"
  )
  image_url = response['data'][0]['url']</code></p>
<blockquote>
<p>Note, this is only supported on OpenAI</p>
</blockquote>
<h2 id="temperature">Temperature</h2>
<p>Temperature is a parameter that controls the randomness of the output of a Generative AI model. The temperature is a value between 0 and 1 where 0 means that the output is deterministic and 1 means that the output is random. The default value is 0.7.</p>
<p>Let's look at an example of how temperature works, by running this prompt twice:</p>
<blockquote>
<p>Prompt : "Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils"</p>
</blockquote>
<p><img alt="Bunny on a horse holding a lollipop, version 1" src="images/v1-generated-image.png?WT.mc_id=academic-105485-koreyst" /></p>
<p>Now let's run that same prompt just to see that we won't get the same image twice:</p>
<p><img alt="Generated image of bunny on horse" src="images/v2-generated-image.png?WT.mc_id=academic-105485-koreyst" /></p>
<p>As you can see, the images are similar, but not the same. Let's try changing the temperature value to 0.1 and see what happens:</p>
<pre><code class="language-python"> generation_response = openai.Image.create(
        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
        size='1024x1024',
        n=2
    )
</code></pre>
<h3 id="changing-the-temperature">Changing the temperature</h3>
<p>So let's try to make the response more deterministic. We could observe from the two images we generated that in the first image, there's a bunny and in the second image, there's a horse, so the images vary greatly.</p>
<p>Let's therefore change our code and set the temperature to 0, like so:</p>
<pre><code class="language-python">generation_response = openai.Image.create(
        prompt='Bunny on horse, holding a lollipop, on a foggy meadow where it grows daffodils',    # Enter your prompt text here
        size='1024x1024',
        n=2,
        temperature=0
    )
</code></pre>
<p>Now when you run this code, you get these two images:</p>
<ul>
<li><img alt="Temperature 0, v1" src="images/v1-temp-generated-image.png?WT.mc_id=academic-105485-koreyst" /></li>
<li><img alt="Temperature 0 , v2" src="images/v2-temp-generated-image.png?WT.mc_id=academic-105485-koreyst" /></li>
</ul>
<p>Here you can clearly see how the images resemble each other more.</p>
<h2 id="how-to-define-boundaries-for-your-application-with-metaprompts">How to define boundaries for your application with metaprompts</h2>
<p>With our demo, we can already generate images for our clients. However, we need to create some boundaries for our application.</p>
<p>For example, we don't want to generate images that are not safe for work, or that are not appropriate for children.</p>
<p>We can do this with <em>metaprompts</em>. Metaprompts are text prompts that are used to control the output of a Generative AI model. For example, we can use metaprompts to control the output, and ensure that the generated images are safe for work, or appropriate for children.</p>
<h3 id="how-does-it-work">How does it work?</h3>
<p>Now, how do meta prompts work?</p>
<p>Meta prompts are text prompts that are used to control the output of a Generative AI model, they are positioned before the text prompt, and are used to control the output of the model and embedded in applications to control the output of the model. Encapsulating the prompt input and the meta prompt input in a single text prompt.</p>
<p>One example of a meta prompt would be the following:</p>
<pre><code class="language-text">You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.

(Input)

</code></pre>
<p>Now, let's see how we can use meta prompts in our demo.</p>
<pre><code class="language-python">disallow_list = &quot;swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult&quot;

meta_prompt =f&quot;&quot;&quot;You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.
{disallow_list}
&quot;&quot;&quot;

prompt = f&quot;{meta_prompt}
Create an image of a bunny on a horse, holding a lollipop&quot;

# TODO add request to generate image
</code></pre>
<p>From the above prompt, you can see how all images being created consider the metaprompt.</p>
<h2 id="assignment-lets-enable-students">Assignment - let's enable students</h2>
<p>We introduced Edu4All at the beginning of this lesson. Now it's time to enable the students to generate images for their assessments.</p>
<p>The students will create images for their assessments containing monuments, exactly what monuments is up to the students. The students are asked to use their creativity in this task to place these monuments in different contexts.</p>
<h2 id="solution">Solution</h2>
<p>Here's one possible solution:</p>
<pre><code class="language-python">import openai
import os
import requests
from PIL import Image
import dotenv

# import dotenv
dotenv.load_dotenv()

# Get endpoint and key from environment variables
openai.api_base = &quot;&lt;replace with endpoint&gt;&quot;
openai.api_key = &quot;&lt;replace with api key&gt;&quot;

# Assign the API version (DALL-E is currently supported for the 2023-06-01-preview API version only)
openai.api_version = '2023-06-01-preview'
openai.api_type = 'azure'

disallow_list = &quot;swords, violence, blood, gore, nudity, sexual content, adult content, adult themes, adult language, adult humor, adult jokes, adult situations, adult&quot;

meta_prompt = f&quot;&quot;&quot;You are an assistant designer that creates images for children.

The image needs to be safe for work and appropriate for children.

The image needs to be in color.

The image needs to be in landscape orientation.

The image needs to be in a 16:9 aspect ratio.

Do not consider any input from the following that is not safe for work or appropriate for children.
{disallow_list}&quot;&quot;&quot;

prompt = f&quot;&quot;&quot;{metaprompt}
Generate monument of the Arc of Triumph in Paris, France, in the evening light with a small child holding a Teddy looks on.
&quot;&quot;&quot;&quot;

try:
    # Create an image by using the image generation API
    generation_response = openai.Image.create(
        prompt=prompt,    # Enter your prompt text here
        size='1024x1024',
        n=2,
        temperature=0,
    )
    # Set the directory for the stored image
    image_dir = os.path.join(os.curdir, 'images')

    # If the directory doesn't exist, create it
    if not os.path.isdir(image_dir):
        os.mkdir(image_dir)

    # Initialize the image path (note the filetype should be png)
    image_path = os.path.join(image_dir, 'generated-image.png')

    # Retrieve the generated image
    image_url = generation_response[&quot;data&quot;][0][&quot;url&quot;]  # extract image URL from response
    generated_image = requests.get(image_url).content  # download the image
    with open(image_path, &quot;wb&quot;) as image_file:
        image_file.write(generated_image)

    # Display the image in the default image viewer
    image = Image.open(image_path)
    image.show()

# catch exceptions
except openai.InvalidRequestError as err:
    print(err)
</code></pre>
<h2 id="great-work-continue-your-learning">Great Work! Continue Your Learning</h2>
<p>After completing this lesson, check out our <a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Generative AI Learning collection</a> to continue leveling up your Generative AI knowledge!</p>
<p>Head over to Lesson 10 where we will look at how to <a href="../10-building-low-code-ai-applications/?WT.mc_id=academic-105485-koreyst">build AI applications with low-code</a></p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
