<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>第二十章：Mistral的模型 - genaibeginner</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "\u7b2c\u4e8c\u5341\u7ae0\uff1aMistral\u7684\u6a21\u578b";
        var mkdocs_page_input_path = "20-mistral\\README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> genaibeginner
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../00-course-setup/translations/cn/">课程介绍和学习环境设置</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../01-introduction-to-genai/translations/cn/">第一章：生成式人工智能和 LLMs 介绍</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../02-exploring-and-comparing-different-llms/translations/cn/">第二章：探索和比较不同的 LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../03-using-generative-ai-responsibly/translations/cn/">第三章：负责任地使用生成式人工智能</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../04-prompt-engineering-fundamentals/translations/cn/">第四章：提示工程基础</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../05-advanced-prompts/translations/cn/">第五章：创建高级的提示工程技巧</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../06-text-generation-apps/translations/cn/">第六章：创建文本生成应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../07-building-chat-applications/translations/cn/">第七章：创建聊天应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../08-building-search-applications/translations/cn/">第八章：创建搜索应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../09-building-image-applications/translations/cn/">第九章：创建图像应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../10-building-low-code-ai-applications/translations/cn/">第十章：创建低代码AI应用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../11-integrating-with-function-calling/translations/cn/">第十一章：集成函数调用</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../12-designing-ux-for-ai-applications/translations/cn/">第十二章：为人工智能应用程序设计用户体验</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13-securing-ai-applications/translations/cn/">第十三章：保护AI应用程序</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../14-the-generative-ai-application-lifecycle/translations/cn/">第十四章：生成式AI应用生命周期</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../15-rag-and-vector-databases/translations/cn/">第十五章：检索增强生成和向量数据库</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../16-open-source-models/translations/tw/">第十六章：开源模型</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../17-ai-agents/translations/tw/">第十七章：AI Agent</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../18-fine-tuning/translations/tw/">第十八章：微调</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../19-slm/">第十九章：SLM模型</a>
                </li>
              </ul>
              <ul class="current">
                <li class="toctree-l1 current"><a class="reference internal current" href="#">第二十章：Mistral的模型</a>
    <ul class="current">
    <li class="toctree-l2"><a class="reference internal" href="#_1">介绍</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mistral_1">Mistral模型</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mistral-large-2-2407">Mistral Large 2 (2407)</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#mistral-large-2rag">使用Mistral Large 2的RAG示例</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mistral-small">Mistral Small</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mistral-smallmistral-large">比较Mistral Small和Mistral Large</a>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#mistral-nemo">Mistral NeMo</a>
        <ul>
    <li class="toctree-l3"><a class="reference internal" href="#_2">比较分词器</a>
    </li>
        </ul>
    </li>
    <li class="toctree-l2"><a class="reference internal" href="#_3">学习并不停下，继续前行</a>
    </li>
    </ul>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../21-meta/">第二十一章：Meta的模型</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">genaibeginner</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">第二十章：Mistral的模型</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="mistral">使用Mistral模型构建</h1>
<h2 id="_1">介绍</h2>
<p>本课程将涵盖:
- 探索不同的Mistral模型
- 理解每个模型的用例和场景
- 代码示例展示每个模型的独特功能</p>
<h2 id="mistral_1">Mistral模型</h2>
<p>在本课程中，我们将探索三种不同的Mistral模型：<strong>Mistral Large</strong>、<strong>Mistral Small</strong> 和 <strong>Mistral Nemo</strong>。</p>
<p>这些模型都可以在Github Model市场上免费获取。本笔记本中的代码将使用这些模型运行代码。更多关于使用Github模型进行<a href="https://docs.github.com/en/github-models/prototyping-with-ai-models?WT.mc_id=academic-105485-koreyst">原型设计与AI模型</a>的详细信息，请查看链接。</p>
<h2 id="mistral-large-2-2407">Mistral Large 2 (2407)</h2>
<p>Mistral Large 2目前是Mistral的旗舰模型，设计用于企业级使用。</p>
<p>该模型是对原始Mistral Large的升级，提供以下提升：
- 更大的上下文窗口 - 128k vs 32k
- 数学和编程任务上更好的性能 - 平均准确率 76.9% vs 60.4%
- 增强的多语言性能 - 支持语言包括英语、法语、德语、西班牙语、意大利语、葡萄牙语、荷兰语、俄语、中文、日语、韩语、阿拉伯语和印地语。</p>
<p>凭借这些特性，Mistral Large擅长
- <em>检索增强生成 (RAG)</em> - 由于更大的上下文窗口
- <em>功能调用</em> - 该模型具有原生功能调用功能，允许与外部工具和API集成。调用可以并行进行或顺序进行。
- <em>代码生成</em> - 该模型在Python、Java、TypeScript和C++的生成方面表现出色。</p>
<h3 id="mistral-large-2rag">使用Mistral Large 2的RAG示例</h3>
<p>在本示例中，我们使用Mistral Large 2在文本文档上运行RAG模式。问题用韩语编写，询问作者在上大学前的活动。</p>
<p>它使用Cohere Embeddings Model来创建文本文档和问题的嵌入。对于此示例，它使用faiss Python包作为向量存储。</p>
<p>发送给Mistral模型的提示包括问题和检索到的与问题相似的文本块。然后模型提供自然语言的回答。</p>
<pre><code class="language-python">pip install faiss-cpu
</code></pre>
<pre><code class="language-python">import requests
import numpy as np
import faiss
import os

from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential
from azure.ai.inference import EmbeddingsClient

endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-large&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = requests.get('https://raw.githubusercontent.com/run-llama/llama_index/main/docs/docs/examples/data/paul_graham/paul_graham_essay.txt')
text = response.text

chunk_size = 2048
chunks = [text[i:i + chunk_size] for i in range(0, len(text), chunk_size)]
len(chunks)

embed_model_name = &quot;cohere-embed-v3-multilingual&quot; 

embed_client = EmbeddingsClient(
        endpoint=endpoint,
        credential=AzureKeyCredential(token)
)

embed_response = embed_client.embed(
    input=chunks,
    model=embed_model_name
)

text_embeddings = []
for item in embed_response.data:
    length = len(item.embedding)
    text_embeddings.append(item.embedding)
text_embeddings = np.array(text_embeddings)

d = text_embeddings.shape[1]
index = faiss.IndexFlatL2(d)
index.add(text_embeddings)

question = &quot;저자가 대학에 오기 전에 주로 했던 두 가지 일은 무엇이었나요?&quot;

question_embedding = embed_client.embed(
    input=[question],
    model=embed_model_name
)

question_embeddings = np.array(question_embedding.data[0].embedding)

D, I = index.search(question_embeddings.reshape(1, -1), k=2) # 距离，索引
retrieved_chunks = [chunks[i] for i in I.tolist()[0]]

prompt = f&quot;&quot;&quot;
上下文信息如下。
---------------------
{retrieved_chunks}
---------------------
根据上下文信息而非已有知识来回答查询。
查询：{question}
回答：
&quot;&quot;&quot;

chat_response = client.complete(
    messages=[
        SystemMessage(content=&quot;您是一位乐于助人的助手。&quot;),
        UserMessage(content=prompt),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(chat_response.choices[0].message.content)
</code></pre>
<h2 id="mistral-small">Mistral Small</h2>
<p>Mistral Small是Mistral高级/企业类别中的另一个模型。顾名思义，该模型是一个小型语言模型（SLM）。使用Mistral Small的优点是：
- 相比Mistral LLMs如Mistral Large和NeMo可节省成本- 价格下降80%
- 低延迟 - 相比Mistral的LLMs响应速度更快
- 灵活 - 可以跨不同环境部署，对所需资源的限制更少</p>
<p>Mistral Small适用于：
- 文本任务，例如摘要、情感分析和翻译
- 由于成本效益高，适用于频繁请求的应用
- 低延迟代码任务，如代码审查和建议</p>
<h2 id="mistral-smallmistral-large">比较Mistral Small和Mistral Large</h2>
<p>为了展示Mistral Small和Large的延迟差异，请运行以下单元。</p>
<p>你应该可以看到响应时间在3-5秒之间的差异。同时注意相同提示下的响应长度和风格。</p>
<pre><code class="language-python">
import os 
endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-small&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        SystemMessage(content=&quot;您是一位乐于助人的编程助手。&quot;),
        UserMessage(content=&quot;你能写一个Python函数来做Fizz Buzz测试吗？&quot;),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(response.choices[0].message.content)

</code></pre>
<pre><code class="language-python">
import os
from azure.ai.inference import ChatCompletionsClient
from azure.ai.inference.models import SystemMessage, UserMessage
from azure.core.credentials import AzureKeyCredential

endpoint = &quot;https://models.inference.ai.azure.com&quot;
model_name = &quot;Mistral-large&quot;
token = os.environ[&quot;GITHUB_TOKEN&quot;]

client = ChatCompletionsClient(
    endpoint=endpoint,
    credential=AzureKeyCredential(token),
)

response = client.complete(
    messages=[
        SystemMessage(content=&quot;您是一位乐于助人的编程助手。&quot;),
        UserMessage(content=&quot;你能写一个Python函数来做Fizz Buzz测试吗？&quot;),
    ],
    temperature=1.0,
    top_p=1.0,
    max_tokens=1000,
    model=model_name
)

print(response.choices[0].message.content)

</code></pre>
<h2 id="mistral-nemo">Mistral NeMo</h2>
<p>与本课讨论的其他两个模型相比，Mistral NeMo是唯一拥有Apache2许可证的免费模型。</p>
<p>它被视为早期Mistral开源LLM（Mistral 7B）的升级版。</p>
<p>NeMo模型的其他一些特点包括：</p>
<ul>
<li>
<p><em>更高效的分词:</em> 该模型使用Tekken分词器而不是更常用的tiktoken。这样可以在更多语言和代码上获得更好的性能。</p>
</li>
<li>
<p><em>微调:</em> 基础模型可以进行微调。这使得在需要微调的用例中具有更大的灵活性。</p>
</li>
<li>
<p><em>原生功能调用</em> - 像Mistral Large一样，该模型在训练中包含了功能调用。使其成为首批具有此功能的开源模型之一。</p>
</li>
</ul>
<h3 id="_2">比较分词器</h3>
<p>在这个示例中，我们将比较Mistral NeMo与Mistral Large在分词方面的处理。</p>
<p>两个示例都采用相同的提示，你应看到NeMo返回的标记数比Mistral Large少。</p>
<pre><code class="language-bash">pip install mistral-common
</code></pre>
<pre><code class="language-python"># 导入所需的包：
from mistral_common.protocol.instruct.messages import (
    UserMessage,
)
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.protocol.instruct.tool_calls import (
    Function,
    Tool,
)
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer

# 加载Mistral分词器

model_name = &quot;open-mistral-nemo&quot;

tokenizer = MistralTokenizer.from_model(model_name)

# 对消息列表进行分词
tokenized = tokenizer.encode_chat_completion(
    ChatCompletionRequest(
        tools=[
            Tool(
                function=Function(
                    name=&quot;get_current_weather&quot;,
                    description=&quot;获取当前天气&quot;,
                    parameters={
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;location&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;城市和州，例如：San Francisco, CA&quot;,
                            },
                            &quot;format&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                                &quot;description&quot;: &quot;温度单位。根据用户位置推断。&quot;,
                            },
                        },
                        &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],
                    },
                )
            )
        ],
        messages=[
            UserMessage(content=&quot;今天巴黎的天气怎么样&quot;),
        ],
        model=model_name,
    )
)
tokens, text = tokenized.tokens, tokenized.text

# 计算标记数量
print(len(tokens))
</code></pre>
<pre><code class="language-python"># 导入所需的包：
from mistral_common.protocol.instruct.messages import (
    UserMessage,
)
from mistral_common.protocol.instruct.request import ChatCompletionRequest
from mistral_common.protocol.instruct.tool_calls import (
    Function,
    Tool,
)
from mistral_common.tokens.tokenizers.mistral import MistralTokenizer

# 加载Mistral分词器

model_name = &quot;mistral-large-latest&quot;

tokenizer = MistralTokenizer.from_model(model_name)

# 对消息列表进行分词
tokenized = tokenizer.encode_chat_completion(
    ChatCompletionRequest(
        tools=[
            Tool(
                function=Function(
                    name=&quot;get_current_weather&quot;,
                    description=&quot;获取当前天气&quot;,
                    parameters={
                        &quot;type&quot;: &quot;object&quot;,
                        &quot;properties&quot;: {
                            &quot;location&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;description&quot;: &quot;城市和州，例如：San Francisco, CA&quot;,
                            },
                            &quot;format&quot;: {
                                &quot;type&quot;: &quot;string&quot;,
                                &quot;enum&quot;: [&quot;celsius&quot;, &quot;fahrenheit&quot;],
                                &quot;description&quot;: &quot;温度单位。根据用户位置推断。&quot;,
                            },
                        },
                        &quot;required&quot;: [&quot;location&quot;, &quot;format&quot;],
                    },
                )
            )
        ],
        messages=[
            UserMessage(content=&quot;今天巴黎的天气怎么样&quot;),
        ],
        model=model_name,
    )
)
tokens, text = tokenized.tokens, tokenized.text

# 计算标记数量
print(len(tokens))
</code></pre>
<h2 id="_3">学习并不停下，继续前行</h2>
<p>完成本节课程后，请查看我们的<a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">生成性AI学习课程</a>继续提升您的生成性AI知识！</p>
              
            </div>
          </div><footer>
    <div class="rst-footer-buttons" role="navigation" aria-label="Footer Navigation">
        <a href="../19-slm/" class="btn btn-neutral float-left" title="第十九章：SLM模型"><span class="icon icon-circle-arrow-left"></span> Previous</a>
        <a href="../21-meta/" class="btn btn-neutral float-right" title="第二十一章：Meta的模型">Next <span class="icon icon-circle-arrow-right"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
      <span><a href="../19-slm/" style="color: #fcfcfc">&laquo; Previous</a></span>
    
    
      <span><a href="../21-meta/" style="color: #fcfcfc">Next &raquo;</a></span>
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
