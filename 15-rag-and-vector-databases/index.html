<!DOCTYPE html>
<html class="writer-html5" lang="en" >
<head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
      <link rel="shortcut icon" href="../img/favicon.ico" />
    <title>Retrieval Augmented Generation (RAG) and Vector Databases - GenAIæ–°æ‰‹å…¥é—¨</title>
    <link rel="stylesheet" href="../css/theme.css" />
    <link rel="stylesheet" href="../css/theme_extra.css" />
        <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/styles/github.min.css" />
    
      <script>
        // Current page data
        var mkdocs_page_name = "Retrieval Augmented Generation (RAG) and Vector Databases";
        var mkdocs_page_input_path = "15-rag-and-vector-databases\\README.md";
        var mkdocs_page_url = null;
      </script>
    
    <!--[if lt IE 9]>
      <script src="../js/html5shiv.min.js"></script>
    <![endif]-->
      <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/11.8.0/highlight.min.js"></script>
      <script>hljs.highlightAll();</script> 
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
    <div class="wy-side-scroll">
      <div class="wy-side-nav-search">
          <a href=".." class="icon icon-home"> GenAIæ–°æ‰‹å…¥é—¨
        </a><div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../search.html" method="get">
      <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" title="Type search term here" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../00-course-setup/translations/cn/">è¯¾ç¨‹ä»‹ç»å’Œå­¦ä¹ ç¯å¢ƒè®¾ç½®</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../01-introduction-to-genai/translations/cn/">ç¬¬ä¸€ç« ï¼šç”Ÿæˆå¼äººå·¥æ™ºèƒ½å’Œ LLMs ä»‹ç»</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../02-exploring-and-comparing-different-llms/translations/cn/">ç¬¬äºŒç« ï¼šæ¢ç´¢å’Œæ¯”è¾ƒä¸åŒçš„ LLMs</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../03-using-generative-ai-responsibly/translations/cn/">ç¬¬ä¸‰ç« ï¼šè´Ÿè´£ä»»åœ°ä½¿ç”¨ç”Ÿæˆå¼äººå·¥æ™ºèƒ½</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../04-prompt-engineering-fundamentals/translations/cn/">ç¬¬å››ç« ï¼šæç¤ºå·¥ç¨‹åŸºç¡€</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../05-advanced-prompts/translations/cn/">ç¬¬äº”ç« ï¼šåˆ›å»ºé«˜çº§çš„æç¤ºå·¥ç¨‹æŠ€å·§</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../06-text-generation-apps/translations/cn/">ç¬¬å…­ç« ï¼šåˆ›å»ºæ–‡æœ¬ç”Ÿæˆåº”ç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../07-building-chat-applications/translations/cn/">ç¬¬ä¸ƒç« ï¼šåˆ›å»ºèŠå¤©åº”ç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../08-building-search-applications/translations/cn/">ç¬¬å…«ç« ï¼šåˆ›å»ºæœç´¢åº”ç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../09-building-image-applications/translations/cn/">ç¬¬ä¹ç« ï¼šåˆ›å»ºå›¾åƒåº”ç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../10-building-low-code-ai-applications/translations/cn/">ç¬¬åç« ï¼šåˆ›å»ºä½ä»£ç AIåº”ç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../11-integrating-with-function-calling/translations/cn/">ç¬¬åä¸€ç« ï¼šé›†æˆå‡½æ•°è°ƒç”¨</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../12-designing-ux-for-ai-applications/translations/cn/">ç¬¬åäºŒç« ï¼šä¸ºäººå·¥æ™ºèƒ½åº”ç”¨ç¨‹åºè®¾è®¡ç”¨æˆ·ä½“éªŒ</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../13-securing-ai-applications/translations/cn/">ç¬¬åä¸‰ç« ï¼šä¿æŠ¤AIåº”ç”¨ç¨‹åº</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../14-the-generative-ai-application-lifecycle/translations/cn/">ç¬¬åå››ç« ï¼šç”Ÿæˆå¼AIåº”ç”¨ç”Ÿå‘½å‘¨æœŸ</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="translations/cn/">ç¬¬åäº”ç« ï¼šæ£€ç´¢å¢å¼ºç”Ÿæˆå’Œå‘é‡æ•°æ®åº“</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../16-open-source-models/translations/tw/">ç¬¬åå…­ç« ï¼šå¼€æºæ¨¡å‹</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../17-ai-agents/translations/tw/">ç¬¬åä¸ƒç« ï¼šAI Agent</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../18-fine-tuning/translations/tw/">ç¬¬åå…«ç« ï¼šå¾®è°ƒ</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../19-slm/">ç¬¬åä¹ç« ï¼šSLMæ¨¡å‹</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../20-mistral/">ç¬¬äºŒåç« ï¼šMistralçš„æ¨¡å‹</a>
                </li>
              </ul>
              <ul>
                <li class="toctree-l1"><a class="reference internal" href="../21-meta/">ç¬¬äºŒåä¸€ç« ï¼šMetaçš„æ¨¡å‹</a>
                </li>
              </ul>
      </div>
    </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">
      <nav class="wy-nav-top" role="navigation" aria-label="Mobile navigation menu">
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="..">GenAIæ–°æ‰‹å…¥é—¨</a>
        
      </nav>
      <div class="wy-nav-content">
        <div class="rst-content"><div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href=".." class="icon icon-home" aria-label="Docs"></a></li>
      <li class="breadcrumb-item active">Retrieval Augmented Generation (RAG) and Vector Databases</li>
    <li class="wy-breadcrumbs-aside">
    </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
            <div class="section" itemprop="articleBody">
              
                <h1 id="retrieval-augmented-generation-rag-and-vector-databases">Retrieval Augmented Generation (RAG) and Vector Databases</h1>
<p><a href="https://aka.ms/gen-ai-lesson15-gh?WT.mc_id=academic-105485-koreyst"><img alt="Retrieval Augmented Generation (RAG) and Vector Databases" src="images/15-lesson-banner.png?WT.mc_id=academic-105485-koreyst" /></a></p>
<p>In the search applications lesson, we briefly learned how to integrate your own data into Large Language Models (LLMs). In this lesson, we will delve further into the concepts of grounding your data in your LLM application, the mechanics of the process and the methods for storing data, including both embeddings and text.</p>
<blockquote>
<p><strong>Video Coming Soon</strong></p>
</blockquote>
<h2 id="introduction">Introduction</h2>
<p>In this lesson we will cover the following:</p>
<ul>
<li>
<p>An introduction to RAG, what it is and why it is used in AI (artificial intelligence).</p>
</li>
<li>
<p>Understanding what vector databases are and creating one for our application.</p>
</li>
<li>
<p>A practical example on how to integrate RAG into an application.</p>
</li>
</ul>
<h2 id="learning-goals">Learning Goals</h2>
<p>After completing this lesson, you will be able to:</p>
<ul>
<li>
<p>Explain the significance of RAG in data retrieval and processing.</p>
</li>
<li>
<p>Setup RAG application and ground your data to an LLM</p>
</li>
<li>
<p>Effective integration of RAG and Vector Databases in LLM Applications.</p>
</li>
</ul>
<h2 id="our-scenario-enhancing-our-llms-with-our-own-data">Our Scenario: enhancing our LLMs with our own data</h2>
<p>For this lesson, we want to add our own notes into the education startup, which allows the chatbot to get more information on the different subjects. Using the notes that we have, learners will be able to study better and understand the different topics, making it easier to revise for their examinations. To create our scenario, we will use:</p>
<ul>
<li>
<p><code>Azure OpenAI:</code> the LLM we will use to create our chatbot</p>
</li>
<li>
<p><code>AI for beginners' lesson on Neural Networks</code>: this will be the data we ground our LLM on</p>
</li>
<li>
<p><code>Azure AI Search</code> and <code>Azure Cosmos DB:</code> vector database to store our data and create a search index</p>
</li>
</ul>
<p>Users will be able to create practice quizzes from their notes, revision flash cards and summarize it to concise overviews. To get started, let us look at what is RAG and how works:</p>
<h2 id="retrieval-augmented-generation-rag">Retrieval Augmented Generation (RAG)</h2>
<p>An LLM powered chatbot processes user prompts to generate responses. It is designed to be interactive and engages with users on a wide array of topics. However, its responses are limited to the context provided and its foundational training data. For instance, GPT-4 knowledge cutoff is September 2021, meaning, it lacks knowledge of events that have occurred after this period. In addition, the data used to train LLMs excludes confidential information such as personal notes or a company's product manual.</p>
<h3 id="how-rags-retrieval-augmented-generation-work">How RAGs (Retrieval Augmented Generation) work</h3>
<p><img alt="drawing showing how RAGs work" src="images/how-rag-works.png?WT.mc_id=academic-105485-koreyst" /></p>
<p>Suppose you want to deploy a chatbot that creates quizzes from your notes, you will require a connection to the knowledge base. This is where RAG comes to the rescue. RAGs operate as follows:</p>
<ul>
<li>
<p><strong>Knowledge base:</strong> Before retrieval, these documents need to be ingested and preprocessed, typically breaking down large documents into smaller chunks, transforming them to text embedding and storing them in a database.</p>
</li>
<li>
<p><strong>User Query:</strong> the user asks a question</p>
</li>
<li>
<p><strong>Retrieval:</strong> When a user asks a question, the embedding model retrieves relevant information from our knowledge base to provide more context that will be incorporated into the prompt.</p>
</li>
<li>
<p><strong>Augmented Generation:</strong> the LLM enhances its response based on the data retrieved. It allows the response generated to be not only based on pre-trained data but also relevant information from the added context. The retrieved data is used to augment the LLM's responses. The LLM then returns an answer to the user's question.</p>
</li>
</ul>
<p><img alt="drawing showing how RAGs architecture" src="images/encoder-decode.png?WT.mc_id=academic-105485-koreyst" /></p>
<p>The architecture for RAGs is implemented using transformers consisting of two parts: an encoder and a decoder. For example, when a user asks a question, the input text 'encoded' into vectors capturing the meaning of words and the vectors are 'decoded' into our document index and generates new text based on the user query. The LLM uses both an encoder-decoder model to generate the output.</p>
<p>Two approaches when implementing RAG according to the proposed paper: <a href="https://arxiv.org/pdf/2005.11401.pdf?WT.mc_id=academic-105485-koreyst">Retrieval-Augmented Generation for Knowledge intensive NLP (natural language processing software) Tasks</a> are:</p>
<ul>
<li>
<p><strong><em>RAG-Sequence</em></strong> using retrieved documents to predict the best possible answer to a user query</p>
</li>
<li>
<p><strong>RAG-Token</strong> using documents to generate the next token, then retrieve them to answer the user's query</p>
</li>
</ul>
<h3 id="why-would-you-use-rags">Why would you use RAGs?</h3>
<ul>
<li>
<p><strong>Information richness:</strong> ensures text responses are up to date and current. It, therefore, enhances performance on domain specific tasks by accessing the internal knowledge base.</p>
</li>
<li>
<p>Reduces fabrication by utilizing <strong>verifiable data</strong> in the knowledge base to provide context to the user queries.</p>
</li>
<li>
<p>It is <strong>cost effective</strong> as they are more economical compared to fine-tuning an LLM</p>
</li>
</ul>
<h2 id="creating-a-knowledge-base">Creating a knowledge base</h2>
<p>Our application is based on our personal data i.e., the Neural Network lesson on AI For Beginners curriculum.</p>
<h3 id="vector-databases">Vector Databases</h3>
<p>A vector database, unlike traditional databases, is a specialized database designed to store, manage and search embedded vectors. It stores numerical representations of documents. Breaking down data to numerical embeddings makes it easier for our AI system to understand and process the data.</p>
<p>We store our embeddings in vector databases as LLMs have a limit of the number of tokens they accept as input. As you cannot pass the entire embeddings to an LLM, we will need to break them down into chunks and when a user asks a question, the embeddings most like the question will be returned together with the prompt. Chunking also reduces costs on the number of tokens passed through an LLM.</p>
<p>Some popular vector databases include Azure Cosmos DB, Clarifyai, Pinecone, Chromadb, ScaNN, Qdrant and DeepLake. You can create an Azure Cosmos DB model using Azure CLI with the following command:</p>
<pre><code class="language-bash">az login
az group create -n &lt;resource-group-name&gt; -l &lt;location&gt;
az cosmosdb create -n &lt;cosmos-db-name&gt; -r &lt;resource-group-name&gt;
az cosmosdb list-keys -n &lt;cosmos-db-name&gt; -g &lt;resource-group-name&gt;
</code></pre>
<h3 id="from-text-to-embeddings">From text to embeddings</h3>
<p>Before we store our data, we will need to convert it to vector embeddings before it is stored in the database. If you are working with large documents or long texts, you can chunk them based on queries you expect. Chunking can be done at sentence level, or at a paragraph level. As chunking derives meanings from the words around them, you can add some other context to a chunk, for example, by adding the document title or including some text before or after the chunk. You can chunk the data as follows:</p>
<pre><code class="language-python">def split_text(text, max_length, min_length):
    words = text.split()
    chunks = []
    current_chunk = []

    for word in words:
        current_chunk.append(word)
        if len(' '.join(current_chunk)) &lt; max_length and len(' '.join(current_chunk)) &gt; min_length:
            chunks.append(' '.join(current_chunk))
            current_chunk = []

    # If the last chunk didn't reach the minimum length, add it anyway
    if current_chunk:
        chunks.append(' '.join(current_chunk))

    return chunks
</code></pre>
<p>Once chunked, we can then embed our text using different embedding models. Some models you can use include: word2vec, ada-002 by OpenAI, Azure Computer Vision and many more. Selecting a model to use will depend on the languages you're using, the type of content encoded (text/images/audio), the size of input it can encode and length of the embedding output.</p>
<p>An example of embedded text using OpenAI's <code>text-embedding-ada-002</code> model is:
<img alt="an embedding of the word cat" src="images/cat.png?WT.mc_id=academic-105485-koreyst" /></p>
<h2 id="retrieval-and-vector-search">Retrieval and Vector Search</h2>
<p>When a user asks a question, the retriever transforms it into a vector using the query encoder, it then searches through our document search index for relevant vectors in the document that are related to the input. Once done, it converts both the input vector and document vectors into text and passes it through the LLM.</p>
<h3 id="retrieval">Retrieval</h3>
<p>Retrieval happens when the system tries to quickly find the documents from the index that satisfy the search criteria. The goal of the retriever is to get documents that will be used to provide context and ground the LLM on your data.</p>
<p>There are several ways to perform search within our database such as:</p>
<ul>
<li>
<p><strong>Keyword search</strong> - used for text searches</p>
</li>
<li>
<p><strong>Semantic search</strong> - uses the semantic meaning of words</p>
</li>
<li>
<p><strong>Vector search</strong> - converts documents from text to vector representations using embedding models. Retrieval will be done by querying the documents whose vector representations are closest to the user question.</p>
</li>
<li>
<p><strong>Hybrid</strong> - a combination of both keyword and vector search.</p>
</li>
</ul>
<p>A challenge with retrieval comes in when there is no similar response to the query in the database, the system will then return the best information they can get, however, you can use tactics like set up the maximum distance for relevance or use hybrid search that combines both keywords and vector search. In this lesson we will use hybrid search, a combination of both vector and keyword search. We will store our data into a dataframe with columns containing the chunks as well as embeddings.</p>
<h3 id="vector-similarity">Vector Similarity</h3>
<p>The retriever will search through the knowledge database for embeddings that are close together, the closest neighbour, as they are texts that are similar. In the scenario a user asks a query, it is first embedded then matched with similar embeddings. The common measurement that is used to find how similar different vectors are is cosine similarity which is based on the angle between two vectors.</p>
<p>We can measure similarity using other alternatives we can use are Euclidean distance which is the straight line between vector endpoints and dot product which measures the sum of the products of corresponding elements of two vectors.</p>
<h3 id="search-index">Search index</h3>
<p>When doing retrieval, we will need to build a search index for our knowledge base before we perform search. An index will store our embeddings and can quickly retrieve the most similar chunks even in a large database. We can create our index locally using:</p>
<pre><code class="language-python">from sklearn.neighbors import NearestNeighbors

embeddings = flattened_df['embeddings'].to_list()

# Create the search index
nbrs = NearestNeighbors(n_neighbors=5, algorithm='ball_tree').fit(embeddings)

# To query the index, you can use the kneighbors method
distances, indices = nbrs.kneighbors(embeddings)
</code></pre>
<h3 id="re-ranking">Re-ranking</h3>
<p>Once you have queried the database, you might need to sort the results from the most relevant. A reranking LLM utilizes Machine Learning to improve the relevance of search results by ordering them from the most relevant. Using Azure AI Search, reranking is done automatically for you using a semantic reranker. An example of how reranking works using nearest neighbours:</p>
<pre><code class="language-python"># Find the most similar documents
distances, indices = nbrs.kneighbors([query_vector])

index = []
# Print the most similar documents
for i in range(3):
    index = indices[0][i]
    for index in indices[0]:
        print(flattened_df['chunks'].iloc[index])
        print(flattened_df['path'].iloc[index])
        print(flattened_df['distances'].iloc[index])
    else:
        print(f&quot;Index {index} not found in DataFrame&quot;)
</code></pre>
<h2 id="bringing-it-all-together">Bringing it all together</h2>
<p>The last step is adding our LLM into the mix to be able to get responses that are grounded on our data. We can implement it as follows:</p>
<pre><code class="language-python">user_input = &quot;what is a perceptron?&quot;

def chatbot(user_input):
    # Convert the question to a query vector
    query_vector = create_embeddings(user_input)

    # Find the most similar documents
    distances, indices = nbrs.kneighbors([query_vector])

    # add documents to query  to provide context
    history = []
    for index in indices[0]:
        history.append(flattened_df['chunks'].iloc[index])

    # combine the history and the user input
    history.append(user_input)

    # create a message object
    messages=[
        {&quot;role&quot;: &quot;system&quot;, &quot;content&quot;: &quot;You are an AI assistant that helps with AI questions.&quot;},
        {&quot;role&quot;: &quot;user&quot;, &quot;content&quot;: history[-1]}
    ]

    # use chat completion to generate a response
    response = openai.chat.completions.create(
        model=&quot;gpt-4&quot;,
        temperature=0.7,
        max_tokens=800,
        messages=messages
    )

    return response.choices[0].message

chatbot(user_input)
</code></pre>
<h2 id="evaluating-our-application">Evaluating our application</h2>
<h3 id="evaluation-metrics">Evaluation Metrics</h3>
<ul>
<li>
<p>Quality of responses supplied ensuring it sounds natural, fluent and human-like</p>
</li>
<li>
<p>Groundedness of the data: evaluating whether the response that came from supplied docs</p>
</li>
<li>
<p>Relevance: evaluating the response matches and is related to the question asked</p>
</li>
<li>
<p>Fluency - whether the response makes sense grammatically</p>
</li>
</ul>
<h2 id="use-cases-for-using-rag-retervival-augmented-generation-and-vector-databases">Use Cases for using RAG (Retervival Augmented Generation) and vector databases</h2>
<p>There are many different use cases where function calls can improve your app like:</p>
<ul>
<li>
<p>Question and Answering: grounding your company data to a chat that can be used by employees to ask questions.</p>
</li>
<li>
<p>Recommendation Systems: where you can create a system that matches the most similar values e.g. movies, restaurants and many more.</p>
</li>
<li>
<p>Chatbot services: you can store chat history and personalize the conversation based on the user data.</p>
</li>
<li>
<p>Image search based on vector embeddings, useful when doing image recognition and anomaly detection.</p>
</li>
</ul>
<h2 id="summary">Summary</h2>
<p>We have covered the fundamental areas of RAG from adding our data to the application, the user query and output. To simplify creation of RAG, you can use frameworks such as Semanti Kernel, Langchain or Autogen.</p>
<h2 id="assignment">Assignment</h2>
<p>To continue your learning of Retrieval Augmented Generation (RAG) you can build:</p>
<ul>
<li>
<p>Build a front-end for the application using the framework of your choice</p>
</li>
<li>
<p>Utilize a framework, either LangChain or Semantic Kernel, and recreate your application.</p>
</li>
</ul>
<p>Congratulations for completing the lesson ğŸ‘.</p>
<h2 id="learning-does-not-stop-here-continue-the-journey">Learning does not stop here, continue the Journey</h2>
<p>After completing this lesson, check out our <a href="https://aka.ms/genai-collection?WT.mc_id=academic-105485-koreyst">Generative AI Learning collection</a> to continue leveling up your Generative AI knowledge!</p>
              
            </div>
          </div><footer>

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
  </div>

  Built with <a href="https://www.mkdocs.org/">MkDocs</a> using a <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
          
        </div>
      </div>

    </section>

  </div>

  <div class="rst-versions" role="note" aria-label="Versions">
  <span class="rst-current-version" data-toggle="rst-current-version">
    
    
    
  </span>
</div>
    <script src="../js/jquery-3.6.0.min.js"></script>
    <script>var base_url = "..";</script>
    <script src="../js/theme_extra.js"></script>
    <script src="../js/theme.js"></script>
      <script src="../search/main.js"></script>
    <script>
        jQuery(function () {
            SphinxRtdTheme.Navigation.enable(true);
        });
    </script>

</body>
</html>
